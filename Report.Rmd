---
title: "Rating Effectiveness of Excercises"
author: "Kevin C. Limburg"
date: "09/20/2014"
output: html_document
---
  
```{r, echo=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, cache=TRUE, comment="", message=FALSE, warning=FALSE, fig.keep='none', results='hide')

```

## Executive Summary

The purpose of this report is to develop a Human Activity Recognition algorithm to categorize the performance of an exercise routine. The dataset comes from Velloso's paper *Qualitative Activity Recognition of Weight Lifting Exercises*[^1]. The five categories are named from *A* to *E* where *A* is properly performing the excercise, while *B*-*E* are four common mistakes. The [website](http://groupware.les.inf.puc-rio.br/har#ixzz3DzVvINyT) where we located the paper had this description of the data, "Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E)."

[^1]:  Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

We used two different methodologies to build the prediction model, recursive partioning (CART)[^2] and random forest[^3]. We found that the model fit using the random forest methodolgy was far more accurate than the recursive partitioning model and correctly classified all twenty test samples.

[^2]: http://cran.r-project.org/web/packages/rpart/rpart.pdf
[^3]: http://cran.r-project.org/web/packages/randomForest/randomForest.pdf

## Exploratory Analysis

```{r loadData}
#download files
source("./scripts/download_pml_data.R")
# setup parallel processing
require(doSNOW)
registerDoSNOW(makeCluster(8, type="SOCK"))
getDoParWorkers()
getDoParName()
getDoParVersion()
#read data
df.train<-read.csv("./data/pml_train.csv",
                   na=c("#DIV/0!","NA"))
df.train<-df.train[,-1]

df.test<-read.csv("./data/pml_test.csv",
                   na=c("#DIV/0!","NA"))
df.test<-df.test[,-1]
library(caret)
allNA<-sapply(df.test,function(x){sum(is.na(x))==length(x)})
#drop all vars that dont appear in test set
df.train.sub<-df.train[,!allNA]

# clean up data
classe.num<-which(names(df.train.sub)=="classe")
summary(df.train.sub)
df.train.sub$cvtd_timestamp<-as.POSIXct(df.train$cvtd_timestamp,
                                    format = "%m/%d/%Y %H:%M")


library(dplyr)
outlier<-which(df.train.sub[,"gyros_forearm_x"] <(-20))
#drop obseveration from data
df.train.sub<-df.train.sub[-outlier,]
summary(df.train.sub)
sapply(df.train.sub,function(x){sum(is.na(x))})

``` 

The data was loaded into R and we did some minor manipulation to handle the NA values, including *#DIV/0!*. We then looked for any missing data.


We chose to only select variables that were non-missing in the test set as there were some summary statistics that were mostly missing from the training set as well. This reduced the number of variables in the dataset from `r ncol(df.train)` to `r ncol(df.train.sub)`. Additionally we found one outlier that we removed, this changed the number of observations in the test set from `r nrow(df.train)` to `r nrow(df.train.sub)`. We then generated boxplots and f-statistics for each variable versus the response, *classe* (*A*-*E*).

```{r explore}
#make boxplots
df.fstat<-data.frame("var.name"=NA,"fstat"=NA)
row.num<-1
for(i in 7:(ncol(df.train.sub)-1)){
    var.name<-names(df.train.sub)[i]
    lm1<-lm(df.train.sub[,i]~df.train.sub$classe)
    lm1.summary<-summary(lm1)
    df.fstat[row.num,1]<-var.name
    df.fstat[row.num,2]<-lm1.summary$fstatistic[1]
    row.num<-row.num+1
    
#     file.name<-paste0("./figures/explore/",
#                       var.name,
#                       ".png")
#     print(file.name)
#     png(filename = file.name,480,480)
# 
#     boxplot(df.train.sub[,i]~df.train.sub$classe,
#             ylab=var.name)
#     dev.off()
}
```

Below we see the top 6 variables ranked by how much the response variable, *classe*, contributes to their variance, as measured by the F-statistic from the linear model $Y~= X{0}+classe*X{1}$
```{r explore2, results='markup', message=TRUE}
df.fstat<-df.fstat%>%arrange(desc(fstat))
head(df.fstat)
```

We next made a scatter plot of the top two variables and color the points by their classification. These two variables alone are not going to be enough to separate the groups as their is alot of overlap between the groups. Many more of these plots were made as well during our exploratory analysis.
```{r plot1, results='markup', message=TRUE, fig.align='center',fig.cap='Scatterplot of forearm pitch versus magnet belt (y) colored by classification group. These two variables were chosen as they had the most variation with regard to classification group.', fig.keep='high', cache=FALSE}
library(ggplot2)
df.plot1<-df.train.sub[,c("classe",df.fstat[1:2,"var.name"])]
ggplot(df.plot1)+
      geom_point(aes(x=magnet_belt_y, y=pitch_forearm, color= classe),alpha=0.7)+
      labs(y="Forearm Pitch", x= "Magnet Belt (y)", color = "Classification",
           title = "Scatterplot of Excercise Classification Groups")
```


## Model Selection

``` {r modelSelect1}
# rpart models
set.seed(1234)
model.rpart.cv10x1<-train(classe~.,data=df.train.sub[,-c(1:6)],method="rpart",
                         trControl = trainControl(method = "cv", 
                                                  number = 10,
                                                  repeats = 1))
set.seed(1234)
model.rpart.bs25x1<-train(classe~.,data=df.train.sub[,-c(1:6)],method="rpart",
                         trControl = trainControl(method = "boot", 
                                                  number = 25,
                                                  repeats = 1))
set.seed(1234)
model.rf.cv10x1 <- train(classe~., data=df.train.sub[,-c(1:6)], method="rf",ntree=10,
                         trControl = trainControl(method = "cv", 
                                                  number = 10,
                                                  repeats = 1))

set.seed(1234)
model.rf.bs25x1 <- train(classe~., data=df.train.sub[,-c(1:6)], method="rf",
                           trControl = trainControl(method = "boot", 
                                                    number = 25,
                                                    repeats = 1),
                           ntree=10)

```

After doing a preliminary exploratory analysis and variable reduction of the data, we began fitting our models. We made heavy use of the package 'caret'[^4] for R for both fitting the models and doing the model diagnostics. We chose two different types of models, a classification tree and a random forest, to use for model fitting.


In addition to the variables that had already been we remove, we chose to ignore the name, date/time, and window variables for our model. We would like it to be as robust as possible to other users at other times. We thought that this information, while potentially useful to the describing the training set, may have led to overfitting.

[^4]: http://cran.r-project.org/web/packages/caret/caret.pdf

For model tuning and validation we used two different approaches, a 10-fold cross validation and a bootstrap with 25 repetition, for both model types. For the random forest models we used only 10 trees in an attempt to reduce the computation time. We allowed the caret package's train function to tune our models and chose the best choice for minimizing out-of-sample error.

## Analysis

```{r analysis}

model.rpart.cv10x1
model.rpart.cv10x1$finalModel
pred.train<-predict(model.rpart.cv10x1$finalModel,
                    newdata=df.train.sub[,-c(1:6)],
                    type="class")
confusionMatrix(pred.train,reference=df.train.sub$classe)

model.rpart.bs25x1
model.rpart.bs25x1$finalModel
pred.train<-predict(model.rpart.bs25x1$finalModel,
                    newdata=df.train.sub[,-c(1:6)],
                    type="class")
confusionMatrix(pred.train,reference=df.train.sub$classe)

# model diagnostics
model.rf.cv10x1
varImps.rf.cv10<-varImp(model.rf.cv10x1)
model.rf.cv10x1$finalModel
pred.train<-predict(model.rf.cv10x1$finalModel,df.train.sub[,-c(1:6)])
confusionMatrix(pred.train,reference=df.train.sub$classe)

```
In the case of our models there was no real difference between the 10-fold cross-validated models versus the bootstrapped model for either the random forest or classification tree. For the remainder of the report we will be discussing the 10-fold cross-validated models.
Below is the classification tree and confusion matrix for our classification tree model.

```{r plot2,fig.align='center',fig.cap='Classification Tree for Excercise Evaluation', fig.keep='high', cache=FALSE, message=TRUE,tidy=TRUE, results='markup'}
library(rattle)
library(caret)
fancyRpartPlot(model.rpart.cv10x1$finalModel, sub = NULL,main = 'Classification Tree for Excercise Evaluation',mar=c(2,1,3,2))

conf.mat.rpart<-confusionMatrix(model.rpart.cv10x1)
conf.mat.rpart
```
Next we will look at the confusion matrix for the random forest model.

```{r rf.final, message=TRUE,tidy=TRUE, results='markup'}
library(caret)
conf.mat.rf<-confusionMatrix(model.rf.cv10x1)
conf.mat.rf
```

We see that the estimated out-of-sample error rate for the random forest model is much lower than that of the classification tree method, `r (100-sum(diag(conf.mat.rf$table)))/100` versus `r (100-sum(diag(conf.mat.rpart$table)))/100`.

```{r} 

#Final Predictions
pred.test.rf<-predict(model.rf.cv10x1$finalModel,newdata=df.test[,!allNA])
pred.test.rpart<-predict(model.rpart.cv10x1$finalModel,newdata=df.test[,!allNA], type= "class")

```
## Conclusion

In conclusion we find that the random forest model does an excellent job properly classifying the exercise evaluation groupings. One large draw back of the random forest methodolgy is how computational intensive it is. Our final random forest model properly classified all twenty test cases, based upon submission score, while the classification tree model implemented with rpart only properly classified `r sum(pred.test.rf==pred.test.rpart)`. Both of these scores are in line with the estimated out-of-sample error rates.
